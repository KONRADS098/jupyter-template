{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "**Type:** Supervised Learning    \n",
    "**Sub-type:** Multi-Class / Multi-Label Classification      \n",
    "**Goal:** Classify data into one of multiple classes or assign multiple labels to data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the target variable based on the goal of the notebook, considering whether it’s a multi-class or multi-label problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the features based on exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into Training, Validation, and Test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{ insert classifier name here }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model on the validation set and adjust hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write down the model’s performance metrics, including accuracy, precision, recall, F1 score, and macro/micro-averaged metrics. Provide interpretations of these metrics.\n",
    "\n",
    "> Metrics for Multi-Class Classification:   \n",
    "> \t•\tAccuracy: The proportion of correctly classified instances.     \n",
    "> \t•\tPrecision: The ratio of true positives to the sum of true and false positives for each class, averaged over all classes.        \n",
    "> \t•\tRecall: The ratio of true positives to the sum of true positives and false negatives for each class, averaged over all classes.     \n",
    "> \t•\tF1 Score: The harmonic mean of precision and recall for each class, averaged over all classes.      \n",
    "> \t•\tConfusion Matrix: A table showing the number of correct and incorrect predictions for each class.       \n",
    "> \t•\tMacro Average: Average of precision, recall, and F1 score across all classes, treating each class equally.      \n",
    "> \t•\tMicro Average: Aggregated performance metrics over all classes, treating each instance equally.     \n",
    ">           \n",
    "> Metrics for Multi-Label Classification:       \n",
    "> \t•\tSubset Accuracy: The proportion of instances where all the true labels are exactly matched by the predicted labels.         \n",
    "> \t•\tPrecision, Recall, F1 Score (per label): Calculated for each label and then averaged (macro or micro).      \n",
    "> \t•\tHamming Loss: The fraction of labels that are incorrectly predicted.        \n",
    "> \t•\tLabel Ranking Average Precision: Measures the ability of the model to rank relevant labels higher than irrelevant ones.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{ insert classifier name here }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model on the validation set and adjust hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write down the model’s performance metrics, including accuracy, precision, recall, F1 score, and macro/micro-averaged metrics. Provide interpretations of these metrics.\n",
    "\n",
    "> Metrics for Multi-Class Classification:   \n",
    "> \t•\tAccuracy: The proportion of correctly classified instances.     \n",
    "> \t•\tPrecision: The ratio of correctly predicted positive observations to the total predicted positive observations.      \n",
    "> \t•\tRecall: The ratio of correctly predicted positive observations to the all observations in actual class.     \n",
    "> \t•\tF1 Score: The weighted average of Precision and Recall.  \n",
    "> \t•\tConfusion Matrix: A table showing the number of correct and incorrect predictions for each class.       \n",
    "> \t•\tMacro Average: Average of precision, recall, and F1 score across all classes, treating each class equally.      \n",
    "> \t•\tMicro Average: Aggregated performance metrics over all classes, treating each instance equally.     \n",
    ">           \n",
    "> Metrics for Multi-Label Classification:       \n",
    "> \t•\tSubset Accuracy: The proportion of instances where all the true labels are exactly matched by the predicted labels.         \n",
    "> \t•\tPrecision, Recall, F1 Score (per label): Calculated for each label and then averaged (macro or micro).      \n",
    "> \t•\tHamming Loss: The fraction of labels that are incorrectly predicted.        \n",
    "> \t•\tLabel Ranking Average Precision: Measures the ability of the model to rank relevant labels higher than irrelevant ones.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Summarize and compare the performance metrics (accuracy, macro/micro average precision, recall, F1 Score, etc.) of the different models. Discuss which model performed the best and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Final Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select the best-performing model and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Report the final metrics for the best model based on the validation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the confusion matrix and detailed classification report for the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For multi-class, look at which classes are misclassified. For multi-label, examine the label-wise performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Metrics for Multi-Label Classification (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze Hamming Loss, Subset Accuracy, and Label Ranking Average Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss the model’s performance in handling multiple labels and the quality of ranking labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Findings and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write a summary of the performance metrics for the final model and discuss its strengths and weaknesses. Provide insights into how the model can be used in practice and suggest next steps for improvement or deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
